{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/ismailaseck/spp-mlp?scriptVersionId=86883422\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-01T21:10:28.712245Z","iopub.execute_input":"2022-02-01T21:10:28.712802Z","iopub.status.idle":"2022-02-01T21:10:28.744077Z","shell.execute_reply.started":"2022-02-01T21:10:28.712684Z","shell.execute_reply":"2022-02-01T21:10:28.743251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import SGD\nimport torch.nn.functional as F \n\nfrom sklearn.impute import KNNImputer # to deal with missing values\n\nfrom sklearn.preprocessing import MinMaxScaler # to rescale the data\n\nfrom seaborn import histplot","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:30.338942Z","iopub.execute_input":"2022-02-01T21:10:30.33982Z","iopub.status.idle":"2022-02-01T21:10:32.941156Z","shell.execute_reply.started":"2022-02-01T21:10:30.33977Z","shell.execute_reply":"2022-02-01T21:10:32.940157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loading the data\n\nand get basic information about it","metadata":{}},{"cell_type":"code","source":"# loading the data\ntrain_original = pd.read_csv(\"/kaggle/input/song-popularity-prediction/train.csv\")\ntest_original = pd.read_csv(\"/kaggle/input/song-popularity-prediction/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/song-popularity-prediction/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:32.946726Z","iopub.execute_input":"2022-02-01T21:10:32.949235Z","iopub.status.idle":"2022-02-01T21:10:33.334184Z","shell.execute_reply.started":"2022-02-01T21:10:32.949192Z","shell.execute_reply":"2022-02-01T21:10:33.33342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.338882Z","iopub.execute_input":"2022-02-01T21:10:33.341126Z","iopub.status.idle":"2022-02-01T21:10:33.375093Z","shell.execute_reply.started":"2022-02-01T21:10:33.34108Z","shell.execute_reply":"2022-02-01T21:10:33.374289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in train_original.columns:\n    c_unique =  np.unique(train_original[c].dropna()) # dropping the nan then counting the number of unique values\n    print(c, len(c_unique))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.380874Z","iopub.execute_input":"2022-02-01T21:10:33.383492Z","iopub.status.idle":"2022-02-01T21:10:33.455789Z","shell.execute_reply.started":"2022-02-01T21:10:33.383453Z","shell.execute_reply":"2022-02-01T21:10:33.455014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So key, audio_mode and time_signature are categorical features. We are going to use embeddings to deal with them and harness the power of multi-layer perceptron. We inform the train_original dataframe that those features are categorical in the following cell. ","metadata":{}},{"cell_type":"code","source":"to_categorical = {'key': 'category', 'audio_mode':'category', 'time_signature':'category'}\n\nfor k in to_categorical:\n    train_original[k].astype('category')\n    test_original[k].astype('category')\n#train_original.info()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.460604Z","iopub.execute_input":"2022-02-01T21:10:33.463181Z","iopub.status.idle":"2022-02-01T21:10:33.479921Z","shell.execute_reply.started":"2022-02-01T21:10:33.463143Z","shell.execute_reply":"2022-02-01T21:10:33.479218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.484105Z","iopub.execute_input":"2022-02-01T21:10:33.486141Z","iopub.status.idle":"2022-02-01T21:10:33.498524Z","shell.execute_reply.started":"2022-02-01T21:10:33.486105Z","shell.execute_reply":"2022-02-01T21:10:33.497671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# dealing with missing values using the KNNImputer of sklearn\n\n1. We will first rescale all the numerical columns to be between -0.5 and 0.5 to have the same order of magnitude when using the KNNImputer.\n\n2. Then we use the KNNImputer to fit the numerical columns using the mean of the nearest neighbors.\n\n3. Then we use the KNNImputer to fit the categorical values using the most represented category. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rescaling\nrescaled_train = train_original.copy()\nrescaled_test = test_original.copy()\nignore_columns = ['id', 'audio_mode', 'time_signature','song_popularity']\nscaler_dict = {}\n\nfor column in train_original.columns:\n    if column not in ignore_columns:\n        print(column)\n        scaler = MinMaxScaler(feature_range=(-0.5,0.5)) \n        transformed_column = scaler.fit_transform(train_original[column].values.reshape(40000,1))\n        rescaled_train[column] = transformed_column.squeeze()\n        rescaled_test[column] = scaler.transform(test_original[column].values.reshape(10000,1))\n        scaler_dict[column] = scaler\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.503042Z","iopub.execute_input":"2022-02-01T21:10:33.505087Z","iopub.status.idle":"2022-02-01T21:10:33.538662Z","shell.execute_reply.started":"2022-02-01T21:10:33.505052Z","shell.execute_reply":"2022-02-01T21:10:33.538047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler_dict","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.542454Z","iopub.execute_input":"2022-02-01T21:10:33.544449Z","iopub.status.idle":"2022-02-01T21:10:33.557141Z","shell.execute_reply.started":"2022-02-01T21:10:33.544414Z","shell.execute_reply":"2022-02-01T21:10:33.556304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rescaled_train","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.560971Z","iopub.execute_input":"2022-02-01T21:10:33.56302Z","iopub.status.idle":"2022-02-01T21:10:33.602092Z","shell.execute_reply.started":"2022-02-01T21:10:33.562983Z","shell.execute_reply":"2022-02-01T21:10:33.601336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.606588Z","iopub.execute_input":"2022-02-01T21:10:33.6069Z","iopub.status.idle":"2022-02-01T21:10:33.650859Z","shell.execute_reply.started":"2022-02-01T21:10:33.606862Z","shell.execute_reply":"2022-02-01T21:10:33.64993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# comment : \nThe only categorical feature having missing values is 'key'. And since 'audio_mode' takes only two values, 0 and 1, it does not matter if it is ordinal or not.\nRegarding the 'time_signature', we assume it is ordinal and hence can be used to compute distance. Furthermore, 'time_signature' takes only 4 values [2,3,4,5] and mostly takes value 3 and 4 and rarely 2 and 5.\nSo we are going to consider them as ordinale and use them to compute distances in the KNNImputer. ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:43:25.755987Z","iopub.execute_input":"2022-01-31T13:43:25.756328Z","iopub.status.idle":"2022-01-31T13:43:25.767172Z","shell.execute_reply.started":"2022-01-31T13:43:25.756293Z","shell.execute_reply":"2022-01-31T13:43:25.765908Z"}}},{"cell_type":"code","source":"#unique, count = np.unique(train_original[\"time_signature\"], return_counts=True)\n#unique, count # change to histogram when sharing \nhistplot(train_original[\"time_signature\"], discrete=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:33.814347Z","iopub.execute_input":"2022-02-01T21:10:33.81464Z","iopub.status.idle":"2022-02-01T21:10:34.178433Z","shell.execute_reply.started":"2022-02-01T21:10:33.814613Z","shell.execute_reply":"2022-02-01T21:10:34.177782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing continuous values","metadata":{}},{"cell_type":"code","source":"\nimputer_continuous = KNNImputer(n_neighbors=15,weights='distance')\nimputed_train = rescaled_train.copy()\nimputed_test = rescaled_test.copy()\n# let the imputation use the song_popularity column and add song_popularity to the test data but\n# set it to 0.5 so that it does not contribute to the imputation. \nimputed_train = imputed_train.drop(columns='id') \nimputed_test = imputed_test.drop(columns='id')\nimputed_test['song_popularity']= 0.5\n\nimputed_train = pd.DataFrame(imputer_continuous.fit_transform(imputed_train), columns = imputed_train.columns)\nimputed_test = pd.DataFrame(imputer_continuous.transform(imputed_test), columns = imputed_test.columns)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:10:34.83963Z","iopub.execute_input":"2022-02-01T21:10:34.840394Z","iopub.status.idle":"2022-02-01T21:11:59.200417Z","shell.execute_reply.started":"2022-02-01T21:10:34.840357Z","shell.execute_reply":"2022-02-01T21:11:59.199661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Imputing the categorical value, 'key'","metadata":{}},{"cell_type":"code","source":"imputer_key = KNNImputer(n_neighbors=1)\nrescaled_train['key'] = train_original['key']\nrescaled_test['key'] = test_original['key']\n\n#print(imputed_train.info())\ntemp = pd.DataFrame(imputer_key.fit_transform(rescaled_train.drop(columns='id')), columns = imputed_train.columns)\nimputed_train['key'] = temp['key']\nimputed_train\n\nrescaled_test['song_popularity']=0.5\ntemp = pd.DataFrame(imputer_key.transform(rescaled_test.drop(columns='id')), columns = imputed_train.columns)\nimputed_test['key'] = temp['key']\nimputed_test = imputed_test.drop(columns='song_popularity') # drop the song popularity column since we do not need it anymore\nimputed_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:11:59.202232Z","iopub.execute_input":"2022-02-01T21:11:59.202561Z","iopub.status.idle":"2022-02-01T21:13:12.042438Z","shell.execute_reply.started":"2022-02-01T21:11:59.202525Z","shell.execute_reply":"2022-02-01T21:13:12.041708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:13:12.043728Z","iopub.execute_input":"2022-02-01T21:13:12.044152Z","iopub.status.idle":"2022-02-01T21:13:12.06808Z","shell.execute_reply.started":"2022-02-01T21:13:12.044114Z","shell.execute_reply":"2022-02-01T21:13:12.067281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_test","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:13:12.070093Z","iopub.execute_input":"2022-02-01T21:13:12.070346Z","iopub.status.idle":"2022-02-01T21:13:12.094583Z","shell.execute_reply.started":"2022-02-01T21:13:12.070313Z","shell.execute_reply":"2022-02-01T21:13:12.093758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the dataset to be used with dataloader","metadata":{}},{"cell_type":"code","source":"new_order = ['song_duration_ms', 'acousticness', 'danceability', 'energy',\n       'instrumentalness', 'liveness', 'loudness',\n       'speechiness', 'tempo', 'audio_valence','key', 'audio_mode', 'time_signature',\n       ]\nreord_imputed_train=imputed_train[new_order]\nreord_imputed_train_label = imputed_train['song_popularity']\nreord_imputed_test = imputed_test[new_order]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:13:12.095949Z","iopub.execute_input":"2022-02-01T21:13:12.096416Z","iopub.status.idle":"2022-02-01T21:13:12.10584Z","shell.execute_reply.started":"2022-02-01T21:13:12.09638Z","shell.execute_reply":"2022-02-01T21:13:12.105059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = torch.from_numpy(reord_imputed_train.values)\ny_train = torch.from_numpy(reord_imputed_train_label.values)\nx_test = torch.from_numpy(reord_imputed_test.values)\ntrain_dataset = TensorDataset(x_train,y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=256,shuffle=True)\ntest_dataset = TensorDataset(x_test)\ntest_loader = DataLoader(test_dataset, batch_size=128,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:21:04.606953Z","iopub.execute_input":"2022-02-01T21:21:04.607336Z","iopub.status.idle":"2022-02-01T21:21:04.613692Z","shell.execute_reply.started":"2022-02-01T21:21:04.607291Z","shell.execute_reply":"2022-02-01T21:21:04.612805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# creating the spp_mlp class, a neural network class to predict \nHopefully predictions are more accurate than tree based models' prediction.  ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:21:05.86942Z","iopub.execute_input":"2022-02-01T21:21:05.869677Z","iopub.status.idle":"2022-02-01T21:21:05.874869Z","shell.execute_reply.started":"2022-02-01T21:21:05.86965Z","shell.execute_reply":"2022-02-01T21:21:05.874166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class spp_mlp(nn.Module):\n    def __init__(self, num_neurons=64,drop_prob=0.00, lrelu_slope=0.00):\n        super(spp_mlp, self).__init__()\n        dim_continuous = 10\n        dim_key_embedding = 4\n        dim_ts_embedding = 2\n        dim_am_embedding = 2\n        dim_mlp_input = dim_continuous + dim_key_embedding + dim_ts_embedding + dim_am_embedding\n        self.key_embedding = nn.Embedding(12,dim_key_embedding).to(device) # 12 different values and each being embedded in a dim_key_embedding-dimensional vector\n        self.time_signature_embedding= nn.Embedding(4,dim_ts_embedding).to(device)\n        self.audio_mode_embedding = nn.Embedding(2,dim_am_embedding).to(device)\n        self.fc1 = nn.Linear(dim_mlp_input,num_neurons)\n        self.fc2 = nn.Linear(num_neurons,num_neurons)\n        self.output = nn.Linear(num_neurons,2)\n        self.dropout = nn.Dropout(p=drop_prob)\n        self.lrelu = nn.LeakyReLU(lrelu_slope)\n    \n    def forward(self,x):\n        continuous = x[:,:10].to(device)\n        categorical = x[:,10:].type(torch.LongTensor).to(device)\n        key_embedding = self.key_embedding(categorical[:,0]).clone().to(device)\n        am_embedding =self.audio_mode_embedding(categorical[:,1]).clone().to(device)\n        ts_embedding = self.time_signature_embedding(categorical[:,2]-2).clone().to(device) # -2 to bring the values from [2,3,4,5] to [0,1,2,3]\n        \n        \n        tensors_list = (continuous,key_embedding,ts_embedding,am_embedding)\n        mlp_input = torch.cat(tensors_list,1).to(device)\n        #print(mlp_input.shape)\n        \n        fc1 = self.fc1(mlp_input)\n        fc1 = self.lrelu(fc1)\n        fc1 = self.dropout(fc1)\n        \n        fc2 = self.fc2(fc1)\n        fc2 = self.lrelu(fc2)\n        fc2 = self.dropout(fc2)\n        \n        logit =self.output(fc2)\n        \n        return logit\n    \n    def train_1epoch(self,train_loader, optimizer, device = 'cpu'):\n        self.train()\n        weight = torch.tensor([1./3,2./3]).double()\n        for batch_x, batch_y in train_loader:\n            logit = self(batch_x.to(device))\n            label = batch_y.type(torch.LongTensor).to(device)\n            \n            \n            #loss = F.cross_entropy(logit,label,weight)\n            loss = F.cross_entropy(logit.to(device),label.to(device))\n            #print(f'loss before backward: {loss}')\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)#https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n            \n            #logit = self(batch_x)\n            #loss = F.cross_entropy(logit,label)\n            #print(f'loss after backward {loss}')\n            \n            \n    def evaluate(self,data_loader,device='cpu'):\n        self.eval()\n        eval_loss =0\n        accuracy = 0\n        w = torch.tensor([1./3,2./3]).double()\n        \n        #with torch.no_grad():\n        for batch_x, batch_y in data_loader:\n            logit = self(batch_x.to(device))\n            label = batch_y.type(torch.LongTensor).to(device)\n            #eval_loss += F.cross_entropy(logit, label, w,  reduction='sum')\n            eval_loss += F.cross_entropy(logit.to(device), label.to(device),  reduction='sum')\n            pred = logit.argmax(dim=int(1)).to(device)\n            accuracy += torch.sum(pred==label).double()\n        n_elements = len(data_loader.dataset)\n        return accuracy/n_elements, eval_loss/n_elements","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:23:35.710856Z","iopub.execute_input":"2022-02-01T23:23:35.711137Z","iopub.status.idle":"2022-02-01T23:23:35.730106Z","shell.execute_reply.started":"2022-02-01T23:23:35.711107Z","shell.execute_reply":"2022-02-01T23:23:35.729427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### training the model \n\n\n","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\n\nmlp = spp_mlp(num_neurons=1024)\nmlp.double()\nmlp = mlp.to(device)\n\n\naccuracy, loss = mlp.evaluate(train_loader)\nprint(f'the accuracy before trainig is : {accuracy} and the average loss is : {loss}')\n\nnum_epoch = 10\noptimizer = AdamW(mlp.parameters(), lr = 5e-5)\nfor i in range(num_epoch):\n    mlp.train_1epoch(train_loader, optimizer)\n    accuracy, loss = mlp.evaluate(train_loader)\n    print(f'epoch : {i} accuracy: {accuracy:.4f} loss: {loss:.4f}')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:26:02.18362Z","iopub.execute_input":"2022-02-01T23:26:02.183978Z","iopub.status.idle":"2022-02-01T23:27:55.240258Z","shell.execute_reply.started":"2022-02-01T23:26:02.18394Z","shell.execute_reply":"2022-02-01T23:27:55.238694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    mlp.train_1epoch(train_loader, optimizer)\n    accuracy, loss = mlp.evaluate(train_loader)\n    #if i%4==0:\n    print(f'epoch : {i} accuracy: {accuracy:.4f} loss: {loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:40:16.973276Z","iopub.execute_input":"2022-02-01T23:40:16.973541Z","iopub.status.idle":"2022-02-01T23:42:08.92605Z","shell.execute_reply.started":"2022-02-01T23:40:16.973506Z","shell.execute_reply":"2022-02-01T23:42:08.925287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute AUC on the training set \nfrom sklearn.metrics import roc_auc_score\n\nlogit = mlp(train_dataset[:][0])\nsoftmax = F.softmax(logit,dim=1).cpu().detach().numpy()\nlabel = train_dataset[:][1]\ntrain_auc = roc_auc_score(label,softmax[:,1])\nprint(train_auc)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:46:53.740314Z","iopub.execute_input":"2022-02-01T23:46:53.741138Z","iopub.status.idle":"2022-02-01T23:46:53.792343Z","shell.execute_reply.started":"2022-02-01T23:46:53.741085Z","shell.execute_reply":"2022-02-01T23:46:53.791532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(mlp.state_dict(), \"/kaggle/working/spp_mlp_embedding.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:43:13.76598Z","iopub.execute_input":"2022-02-01T23:43:13.766507Z","iopub.status.idle":"2022-02-01T23:43:13.787319Z","shell.execute_reply.started":"2022-02-01T23:43:13.766471Z","shell.execute_reply":"2022-02-01T23:43:13.786625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optimizer.param_groups[0]['lr'] *= 0.75\n #print(optimizer.param_groups[0]['lr'])","metadata":{"execution":{"iopub.status.busy":"2022-02-01T21:28:26.369337Z","iopub.execute_input":"2022-02-01T21:28:26.369607Z","iopub.status.idle":"2022-02-01T21:28:26.374521Z","shell.execute_reply.started":"2022-02-01T21:28:26.36957Z","shell.execute_reply":"2022-02-01T21:28:26.373693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit = mlp(test_dataset[:][0])\nsoftmax = F.softmax(logit).cpu().detach().numpy()\nsoftmax, logit","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:09:31.764271Z","iopub.execute_input":"2022-02-01T23:09:31.764532Z","iopub.status.idle":"2022-02-01T23:09:31.782318Z","shell.execute_reply.started":"2022-02-01T23:09:31.764504Z","shell.execute_reply":"2022-02-01T23:09:31.781629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission \nsubmission = pd.read_csv(\"../input/song-popularity-prediction/sample_submission.csv\")\n\n\nlogit = mlp(test_dataset[:][0])\nsoftmax = F.softmax(logit).cpu().detach().numpy()\n#label = test_dataset[:][1]\n\nsubmission['song_popularity'] = softmax[:,1]\nsubmission\n#submission.to_csv(f\"sample_submission_{train_auc:.2f}_on_train.csv\", index=False)\nsubmission.to_csv(\"sample_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T23:47:00.028316Z","iopub.execute_input":"2022-02-01T23:47:00.028573Z","iopub.status.idle":"2022-02-01T23:47:00.079057Z","shell.execute_reply.started":"2022-02-01T23:47:00.028544Z","shell.execute_reply":"2022-02-01T23:47:00.078328Z"},"trusted":true},"execution_count":null,"outputs":[]}]}